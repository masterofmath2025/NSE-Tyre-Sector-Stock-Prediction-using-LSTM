{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_BFM_LSTM_single_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye8zDP6vqjtg",
        "outputId": "67379f27-3b93-4437-f8be-98966581dccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "## LSTM Network\n",
        "import time\n",
        "import pandas as pd\n",
        "#### Parameters\n",
        "\n",
        "# For Training Data\n",
        "start = '2015-01-01'  ## FIX THIS\n",
        "end = '2020-10-13'  ## LAST TRAINING DATE + 1\n",
        "stock = 'APOLLOTYRE.NS'    ## Choose one of ['BALKRISIND.NS', 'APOLLOTYRE.NS', 'CEATLTD.NS', 'JKTYRE.NS', 'TVSSRICHAK.NS', 'PTL.NS', 'ELGIRUBCO.NS']\n",
        "# feature = 'High'  ## Select one of ['Open'; 'High' ; 'Low' ; 'Close'] in STRINGS\n",
        "\n",
        "## For Creating dataset\n",
        "time_step = 100     ## No. of days to be 'memorized' by model\n",
        "\n",
        "## For forecasting\n",
        "forecast_days = 11   ## No. of days to be forecasted after training end date\n",
        "\n",
        "## For Verification with Actual Data\n",
        "start_ver = '2020-10-7'  ## PREDICTION START DATE = (LAST TRAINING DATE - 2)\n",
        "end_ver = '2020-10-10'  ## PREDICTION END DATE(TODAY'S DATE)\n",
        "\n",
        "## Initialize Predictions DataFrame\n",
        "Predictions_All = pd.DataFrame()\n",
        "\n",
        "## START\n",
        "main_tic = time.perf_counter()\n",
        "# Install yfinance package. \n",
        "!pip install yfinance \n",
        " \n",
        "# Import yfinance \n",
        "import yfinance as yf   \n",
        " \n",
        "ticks = ['BALKRISIND.NS', 'APOLLOTYRE.NS', 'CEATLTD.NS', 'JKTYRE.NS', 'TVSSRICHAK.NS', 'PTL.NS', 'ELGIRUBCO.NS']\n",
        "labels = ['Balkrishna Industries', 'Apollo', 'CEAT', 'JKTyre', 'TVS', 'PTL', 'Elgi Rubber']\n",
        "colours = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'black']\n",
        "\n",
        "\n",
        "# # Plot the close prices \n",
        "# import matplotlib.pyplot as plt \n",
        "# plt.figure(figsize=(20,20)) \n",
        "# plt.subplot(1,1,1)\n",
        "\n",
        "# for i in range(len(ticks)):\n",
        "#   data = yf.download(ticks[i],'2015-01-01','2020-10-05') \n",
        "#   plt.plot(data.Close, color = colours[i], label = labels[i], linewidth=4.0) \n",
        "# plt.legend(loc=2, prop={'size': 20})\n",
        "# plt.xticks(size=20)\n",
        "# plt.yticks(size=20)\n",
        "# plt.title(\"Stock Prices for companies in the Tyre Sector in NSE from 1 January 2015\", size=20)\n",
        "# plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = yf.download(stock,start,end) \n",
        "data.reset_index(inplace=True)\n",
        "data2 = data[['Date', 'Open', 'High', 'Low', 'Close']]\n",
        "print(data2.tail())\n",
        "\n",
        "for feature in ['Open', 'High', 'Low', 'Close']:\n",
        "    print(\"#################################### \" + feature + \" ###########################################\")\n",
        "    tic = time.perf_counter()\n",
        "    df1 = data2[feature]\n",
        "\n",
        "    # df1\n",
        "\n",
        "    ### LSTM are sensitive to the scale of the data. so we apply MinMax scaler\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler=MinMaxScaler(feature_range=(0,1))\n",
        "    df1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
        "\n",
        "    # print(df1.shape,df1)\n",
        "\n",
        "    ##splitting dataset into train and test split\n",
        "    training_size=int(len(df1)*0.65)\n",
        "    test_size=len(df1)-training_size\n",
        "    train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]\n",
        "\n",
        "    # training_size,test_size\n",
        "\n",
        "    import numpy\n",
        "    # convert an array of values into a dataset matrix\n",
        "\n",
        "    def create_dataset(dataset, time_step=1):\n",
        "      dataX, dataY = [], []\n",
        "      for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + time_step, 0])\n",
        "      return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "    # reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
        "    time_step = time_step\n",
        "\n",
        "    X_train, y_train = create_dataset(train_data, time_step)\n",
        "    X_test, ytest = create_dataset(test_data, time_step)\n",
        "\n",
        "    # print(X_train.shape), print(y_train.shape)\n",
        "\n",
        "    # print(X_test.shape), print(ytest.shape)\n",
        "\n",
        "    ## reshape input to be [samples, time steps, features] which is required for LSTM\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "    ### Create the Stacked LSTM model\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    from tensorflow.keras.layers import LSTM\n",
        "    # print(tf.__version__)\n",
        "\n",
        "    from keras.callbacks import History \n",
        "    history = History()\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(LSTM(50,return_sequences=True,input_shape=(X_train.shape[1],1)))\n",
        "    model.add(LSTM(50,return_sequences=True))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "\n",
        "    #model.summary()\n",
        "\n",
        "    hist = model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1, callbacks=[history])\n",
        "\n",
        "    ### Lets Do the prediction and check performance metrics\n",
        "    train_predict=model.predict(X_train)\n",
        "    test_predict=model.predict(X_test)\n",
        "\n",
        "    ##Transformback to original form\n",
        "    train_predict=scaler.inverse_transform(train_predict)\n",
        "    test_predict=scaler.inverse_transform(test_predict)\n",
        "\n",
        "    ### Calculate RMSE performance metrics\n",
        "    import math\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    print(\"Train MSE: \", math.sqrt(mean_squared_error(y_train,train_predict)))    #Should be 217.44307908665783\n",
        "\n",
        "    ### Test Data RMSE    #Should be 162.29651852385174\n",
        "    print(\"Test MSE: \", math.sqrt(mean_squared_error(ytest,test_predict)))\n",
        "\n",
        "    # ### Plotting \n",
        "    # # shift train predictions for plotting\n",
        "    # look_back=100\n",
        "    # trainPredictPlot = numpy.empty_like(df1)\n",
        "    # trainPredictPlot[:, :] = np.nan\n",
        "    # trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
        "    # # shift test predictions for plotting\n",
        "    # testPredictPlot = numpy.empty_like(df1)\n",
        "    # testPredictPlot[:, :] = numpy.nan\n",
        "    # testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n",
        "    # # plot baseline and predictions\n",
        "    # plt.plot(scaler.inverse_transform(df1), label='Original Data')\n",
        "    # plt.plot(trainPredictPlot, label='Trained on')\n",
        "    # plt.plot(testPredictPlot, label='Prediction')\n",
        "    # plt.show()\n",
        "\n",
        "    # len(test_data)\n",
        "\n",
        "    x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
        "    # x_input.shape\n",
        "\n",
        "    temp_input=list(x_input)\n",
        "    temp_input=temp_input[0].tolist()\n",
        "\n",
        "    # demonstrate prediction for next 10 days\n",
        "    from numpy import array\n",
        "\n",
        "    forecast_days = forecast_days\n",
        "    n_steps = time_step\n",
        "\n",
        "    lst_output=[]\n",
        "    print(\"Normalized Predictions:\\n\")\n",
        "\n",
        "    i=0\n",
        "    while(i<forecast_days):\n",
        "        \n",
        "        if(len(temp_input)>time_step):\n",
        "            #print(temp_input)\n",
        "            x_input=np.array(temp_input[1:])\n",
        "            #print(\"{} day input {}\".format(i,x_input))\n",
        "            x_input=x_input.reshape(1,-1)\n",
        "            x_input = x_input.reshape((1, n_steps, 1))\n",
        "            #print(x_input)\n",
        "            yhat = model.predict(x_input, verbose=0)\n",
        "            # print(\"{} day output: {}\".format(i+1,yhat[0][0]))     ## remove this only\n",
        "            temp_input.extend(yhat[0].tolist())\n",
        "            temp_input=temp_input[1:]\n",
        "            #print(temp_input)\n",
        "            lst_output.extend(yhat.tolist())\n",
        "            i=i+1\n",
        "        else:\n",
        "            x_input = x_input.reshape((1, n_steps,1))\n",
        "            yhat = model.predict(x_input, verbose=0)\n",
        "            # print(\"1 day output: {}\".format(yhat[0][0]))    ## remove this only\n",
        "            temp_input.extend(yhat[0].tolist())\n",
        "            #print(len(temp_input))\n",
        "            lst_output.extend(yhat.tolist())\n",
        "            i=i+1\n",
        "        \n",
        "    #print(lst_output)\n",
        "\n",
        "    model.save(\"saved_model.h5\")\n",
        "\n",
        "    day_new=np.arange(1,time_step+1)\n",
        "    day_pred=np.arange(time_step,time_step + forecast_days)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # len(df1)\n",
        "\n",
        "    Predictions = scaler.inverse_transform(lst_output)\n",
        "    # plt.plot(day_new,scaler.inverse_transform(df1[len(df1)-time_step:]), label='Days for which model was trained')\n",
        "    # plt.plot(day_pred,Predictions, label='Predictions based on that data') \n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    df3=df1.tolist()\n",
        "    df3.extend(lst_output)\n",
        "    df3=scaler.inverse_transform(df3).tolist()\n",
        "    # plt.plot(df3)\n",
        "    # plt.axvline(x=len(df3)-len(Predictions), ls=':', c='black')\n",
        "    # plt.title(\"Entire graph along with predictions\")\n",
        "\n",
        "    \"\"\"## Verify Actual Data\"\"\"\n",
        "\n",
        "    Predictions = Predictions.tolist()\n",
        "\n",
        "    # start_ver = start_ver ## PREDICTION START DATE = LAST TRAINING DATE - 2\n",
        "    # end_ver = end_ver  ## PREDICTION END DATE(TODAY'S DATE)\n",
        "\n",
        "    # data3 = yf.download(stock,start_ver,end_ver) \n",
        "    # data3.reset_index(inplace=True)\n",
        "    # data3 = data3[['Date', 'Open', 'High', 'Low', 'Close']]\n",
        "    # print(data3) # Actual data\n",
        "    # print(Predictions)\n",
        "\n",
        "    # plt.figure(figsize=(12,6))\n",
        "    # plt.plot(day_new,scaler.inverse_transform(df1[len(df1)-time_step:]), label='Days for which model was trained')\n",
        "    # plt.plot(day_pred,Predictions, label='Predictions based on that data') \n",
        "    # plt.plot(day_pred[:len(data3[feature])],data3[[feature]], c='magenta', label='Actual Values')  ## MAKE SURE LENGTH OF DATA3 IS SAME AS LENGTH OF PREDICTIONS\n",
        "    # plt.legend()\n",
        "    # plt.title(\"Predictions and Actual Data\")\n",
        "    # plt.show()\n",
        "\n",
        "    \"\"\"### Accuracy\"\"\"\n",
        "\n",
        "    # from statsmodels.tools.eval_measures import rmse\n",
        "\n",
        "    # def accuracy(y1,y2):\n",
        "        \n",
        "    #     accuracy_df=pd.DataFrame()\n",
        "        \n",
        "    #     rms_error = np.round(rmse(y1, y2),1)\n",
        "        \n",
        "    #     map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1)\n",
        "              \n",
        "    #     accuracy_df=accuracy_df.append({\"RMSE\":rms_error, \"%MAPE\": map_error}, ignore_index=True)\n",
        "        \n",
        "    #     return accuracy_df\n",
        "\n",
        "    # Error = accuracy(data3.Close, Predictions[:len(data3.Close)])\n",
        "    # print(\"Prediction for Day 1 = {}\".format(Predictions[0]))\n",
        "    # print(\"\\nActual {} for Day 1 = {}\".format(feature, data3[feature][0]))\n",
        "    # print(\"\\nError is :\\n\",Error)\n",
        "    # print(\"\\nAccuracy for prediction day 1 is: {} %\".format(100 - np.abs((Predictions[0] - data3[feature][0]) / data3[feature][0] ) * 100))\n",
        "\n",
        "    # Predictions_short = [i[0] for i in Predictions[:len(data3[feature])]]\n",
        "    # # Predictions_short\n",
        "\n",
        "    # # data3[feature]\n",
        "\n",
        "    # accuracy(Predictions_short, data3[feature])\n",
        "\n",
        "    # for i in range(len(data3[feature])):\n",
        "    #     print(\"\\nAccuracy for prediction day {} is: {} %\".format(i+1, 100 - np.abs((Predictions_short[i] - data3[feature][i]) / data3[feature][i] ) * 100))\n",
        "\n",
        "    # print(hist.history['loss'])\n",
        "    Predictions_All[feature] = Predictions\n",
        "\n",
        "    toc = time.perf_counter()\n",
        "    print(\"Time for execution: \", toc - tic)\n",
        "    \n",
        "\n",
        "main_toc = time.perf_counter()\n",
        "print(\"Total time for all executions: \", main_toc - main_tic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/37/d420b7fdc9a550bd29b8cfeacff3b38502d9600b09d7dfae9a69e623b891/lxml-4.5.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=d3136ef316e8602b675d6626428694729144f8a170f1b1dc14ee7069e4eead5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.5.2 yfinance-0.1.55\n",
            "[*********************100%***********************]  1 of 1 completed"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyj5ukhMbjMp"
      },
      "source": [
        "Predictions_All"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkqNEplCMdQc"
      },
      "source": [
        "with pd.ExcelWriter('APOLLO_12Oct.xlsx') as writer:\n",
        "    Predictions_All.to_excel(writer, sheet_name=stock)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JjSq0rPUjlq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5grj3_9cUjdj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}